{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Primitives and Shallow Networks\n",
        "\n",
        "You are part of a research team tasked with migrating traditional NLP methods (like N-Grams) to modern neural network architectures. Before building complex models like RNNs, you must first master the fundamental mathematical components that allow any deep learning model to learn.\n",
        "\n",
        "Your primary goal is to implement and visualize the key functions (Activation, Loss, Optimization) and then integrate them into a simple, fully-connected (shallow) network using a modern framework (PyTorch).\n"
      ],
      "metadata": {
        "id": "wWQKjQA0eW3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import required libraries"
      ],
      "metadata": {
        "id": "JMM7bx4Fi0GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "DlVMrXZ_iiWy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tasks\n",
        "\n",
        "### Task 1: Activation Function Implementation (Sigmoid)\n",
        "The activation function is crucial for introducing non-linearity into the network, allowing it to learn complex patterns.\n",
        "Implement the `sigmoid(x)` function using numpy:\n",
        "\n",
        " - Generate input values  from -10 to 10 and calculate the corresponding Sigmoid output.\n",
        " - Plot the Sigmoid Activation Function using matplotlib to visualize its non-linear S-shape and saturation behavior."
      ],
      "metadata": {
        "id": "S9GUJp7ui8Hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "x = np.linspace(-10, 10, 100)\n",
        "y_sigmoid = sigmoid(x)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, y_sigmoid, label='Sigmoid', color='blue')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Sigmoid(x)')\n",
        "plt.title('Sigmoid Activation Function')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "mOMIbLkni2W6",
        "outputId": "fbe9238f-78c6-45f4-8c5a-afcfa87d2c71"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2: Loss Function Definition (Mean Squared Error)\n",
        "The loss function quantifies the difference between the model's prediction and the true value.\n",
        " - Define the loss function `def loss_function(x)`, which is $ y = x^2$\n",
        " - Deinfe the Gradiant function `def gradiant(x)`, which is $ y = 2x $\n",
        "\n",
        " - Implement and Visualise the Gradient Descent Algorithm Implement a generic `gradient_descent(loss_function, gradient, start_x, learning_rate, iterations)` function.\n",
        " - Use the provided `loss_function(x)` function and its gradient to simulate the iterative update process.\n",
        " - Run the optimizer for a set number of iterations (e.g., 100), printing the x value, Loss, and Gradient at each step to demonstrate convergence to the minimum.\n"
      ],
      "metadata": {
        "id": "eDtwWEZwkmrg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "277b5a2d"
      },
      "source": [
        "#### Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ae0f5891"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10aed5e4"
      },
      "source": [
        "#### Define Loss Function and Gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "58cc3eef"
      },
      "outputs": [],
      "source": [
        "# Define a simple quadratic loss function: y = x^2\n",
        "def loss_function(x):\n",
        "    return x**2\n",
        "\n",
        "# Derivative of the loss function: dy/dx = 2x\n",
        "def gradient(x):\n",
        "    return 2*x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gardient Descent Formulae: new = old - learning rate * slope\n"
      ],
      "metadata": {
        "id": "FuaxaKtPiBvZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfd92187"
      },
      "source": [
        "#### Gradient Descent Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4deae024"
      },
      "outputs": [],
      "source": [
        "# Gradient Descent Algorithm\n",
        "def gradient_descent(starting_point, learning_rate, iterations):\n",
        "  x = starting_point  # Initial value\n",
        "  history = []        # To store x values and corresponding loss\n",
        "\n",
        "\n",
        "  # Update x using gradient descent formula\n",
        "  for i in range(iterations):\n",
        "      loss = loss_function(x)\n",
        "      gradient_value = gradient(x)\n",
        "      x = x - learning_rate * gradient_value\n",
        "      history.append((x, loss))\n",
        "      print(f\"Iteration {i+1}: x = {x:.4f}, Loss = {loss:.4f}, Gradient = {gradient_value:.4f}\")\n",
        "\n",
        "\n",
        "  return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e93051cb"
      },
      "source": [
        "#### Parameters for Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "928009ad"
      },
      "outputs": [],
      "source": [
        "starting_point = 10  # Starting point for x\n",
        "learning_rate = 0.08 # Step size for gradient descent\n",
        "iterations = 300      # Number of iterations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d18ce336"
      },
      "source": [
        "#### Run Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "766e9d7c",
        "outputId": "75a53fda-7fbc-416a-dc65-94feb520d1f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: x = 8.4000, Loss = 100.0000, Gradient = 20.0000\n",
            "Iteration 2: x = 7.0560, Loss = 70.5600, Gradient = 16.8000\n",
            "Iteration 3: x = 5.9270, Loss = 49.7871, Gradient = 14.1120\n",
            "Iteration 4: x = 4.9787, Loss = 35.1298, Gradient = 11.8541\n",
            "Iteration 5: x = 4.1821, Loss = 24.7876, Gradient = 9.9574\n",
            "Iteration 6: x = 3.5130, Loss = 17.4901, Gradient = 8.3642\n",
            "Iteration 7: x = 2.9509, Loss = 12.3410, Gradient = 7.0260\n",
            "Iteration 8: x = 2.4788, Loss = 8.7078, Gradient = 5.9018\n",
            "Iteration 9: x = 2.0822, Loss = 6.1442, Gradient = 4.9575\n",
            "Iteration 10: x = 1.7490, Loss = 4.3354, Gradient = 4.1643\n",
            "Iteration 11: x = 1.4692, Loss = 3.0590, Gradient = 3.4980\n",
            "Iteration 12: x = 1.2341, Loss = 2.1585, Gradient = 2.9383\n",
            "Iteration 13: x = 1.0366, Loss = 1.5230, Gradient = 2.4682\n",
            "Iteration 14: x = 0.8708, Loss = 1.0746, Gradient = 2.0733\n",
            "Iteration 15: x = 0.7315, Loss = 0.7583, Gradient = 1.7416\n",
            "Iteration 16: x = 0.6144, Loss = 0.5350, Gradient = 1.4629\n",
            "Iteration 17: x = 0.5161, Loss = 0.3775, Gradient = 1.2288\n",
            "Iteration 18: x = 0.4335, Loss = 0.2664, Gradient = 1.0322\n",
            "Iteration 19: x = 0.3642, Loss = 0.1880, Gradient = 0.8671\n",
            "Iteration 20: x = 0.3059, Loss = 0.1326, Gradient = 0.7283\n",
            "Iteration 21: x = 0.2570, Loss = 0.0936, Gradient = 0.6118\n",
            "Iteration 22: x = 0.2158, Loss = 0.0660, Gradient = 0.5139\n",
            "Iteration 23: x = 0.1813, Loss = 0.0466, Gradient = 0.4317\n",
            "Iteration 24: x = 0.1523, Loss = 0.0329, Gradient = 0.3626\n",
            "Iteration 25: x = 0.1279, Loss = 0.0232, Gradient = 0.3046\n",
            "Iteration 26: x = 0.1075, Loss = 0.0164, Gradient = 0.2559\n",
            "Iteration 27: x = 0.0903, Loss = 0.0115, Gradient = 0.2149\n",
            "Iteration 28: x = 0.0758, Loss = 0.0081, Gradient = 0.1805\n",
            "Iteration 29: x = 0.0637, Loss = 0.0057, Gradient = 0.1517\n",
            "Iteration 30: x = 0.0535, Loss = 0.0041, Gradient = 0.1274\n",
            "Iteration 31: x = 0.0449, Loss = 0.0029, Gradient = 0.1070\n",
            "Iteration 32: x = 0.0378, Loss = 0.0020, Gradient = 0.0899\n",
            "Iteration 33: x = 0.0317, Loss = 0.0014, Gradient = 0.0755\n",
            "Iteration 34: x = 0.0266, Loss = 0.0010, Gradient = 0.0634\n",
            "Iteration 35: x = 0.0224, Loss = 0.0007, Gradient = 0.0533\n",
            "Iteration 36: x = 0.0188, Loss = 0.0005, Gradient = 0.0448\n",
            "Iteration 37: x = 0.0158, Loss = 0.0004, Gradient = 0.0376\n",
            "Iteration 38: x = 0.0133, Loss = 0.0002, Gradient = 0.0316\n",
            "Iteration 39: x = 0.0111, Loss = 0.0002, Gradient = 0.0265\n",
            "Iteration 40: x = 0.0094, Loss = 0.0001, Gradient = 0.0223\n",
            "Iteration 41: x = 0.0079, Loss = 0.0001, Gradient = 0.0187\n",
            "Iteration 42: x = 0.0066, Loss = 0.0001, Gradient = 0.0157\n",
            "Iteration 43: x = 0.0055, Loss = 0.0000, Gradient = 0.0132\n",
            "Iteration 44: x = 0.0047, Loss = 0.0000, Gradient = 0.0111\n",
            "Iteration 45: x = 0.0039, Loss = 0.0000, Gradient = 0.0093\n",
            "Iteration 46: x = 0.0033, Loss = 0.0000, Gradient = 0.0078\n",
            "Iteration 47: x = 0.0028, Loss = 0.0000, Gradient = 0.0066\n",
            "Iteration 48: x = 0.0023, Loss = 0.0000, Gradient = 0.0055\n",
            "Iteration 49: x = 0.0019, Loss = 0.0000, Gradient = 0.0046\n",
            "Iteration 50: x = 0.0016, Loss = 0.0000, Gradient = 0.0039\n",
            "Iteration 51: x = 0.0014, Loss = 0.0000, Gradient = 0.0033\n",
            "Iteration 52: x = 0.0012, Loss = 0.0000, Gradient = 0.0027\n",
            "Iteration 53: x = 0.0010, Loss = 0.0000, Gradient = 0.0023\n",
            "Iteration 54: x = 0.0008, Loss = 0.0000, Gradient = 0.0019\n",
            "Iteration 55: x = 0.0007, Loss = 0.0000, Gradient = 0.0016\n",
            "Iteration 56: x = 0.0006, Loss = 0.0000, Gradient = 0.0014\n",
            "Iteration 57: x = 0.0005, Loss = 0.0000, Gradient = 0.0011\n",
            "Iteration 58: x = 0.0004, Loss = 0.0000, Gradient = 0.0010\n",
            "Iteration 59: x = 0.0003, Loss = 0.0000, Gradient = 0.0008\n",
            "Iteration 60: x = 0.0003, Loss = 0.0000, Gradient = 0.0007\n",
            "Iteration 61: x = 0.0002, Loss = 0.0000, Gradient = 0.0006\n",
            "Iteration 62: x = 0.0002, Loss = 0.0000, Gradient = 0.0005\n",
            "Iteration 63: x = 0.0002, Loss = 0.0000, Gradient = 0.0004\n",
            "Iteration 64: x = 0.0001, Loss = 0.0000, Gradient = 0.0003\n",
            "Iteration 65: x = 0.0001, Loss = 0.0000, Gradient = 0.0003\n",
            "Iteration 66: x = 0.0001, Loss = 0.0000, Gradient = 0.0002\n",
            "Iteration 67: x = 0.0001, Loss = 0.0000, Gradient = 0.0002\n",
            "Iteration 68: x = 0.0001, Loss = 0.0000, Gradient = 0.0002\n",
            "Iteration 69: x = 0.0001, Loss = 0.0000, Gradient = 0.0001\n",
            "Iteration 70: x = 0.0001, Loss = 0.0000, Gradient = 0.0001\n",
            "Iteration 71: x = 0.0000, Loss = 0.0000, Gradient = 0.0001\n",
            "Iteration 72: x = 0.0000, Loss = 0.0000, Gradient = 0.0001\n",
            "Iteration 73: x = 0.0000, Loss = 0.0000, Gradient = 0.0001\n",
            "Iteration 74: x = 0.0000, Loss = 0.0000, Gradient = 0.0001\n",
            "Iteration 75: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 76: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 77: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 78: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 79: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 80: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 81: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 82: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 83: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 84: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 85: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 86: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 87: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 88: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 89: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 90: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 91: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 92: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 93: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 94: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 95: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 96: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 97: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 98: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 99: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 100: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 101: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 102: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 103: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 104: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 105: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 106: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 107: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 108: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 109: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 110: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 111: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 112: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 113: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 114: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 115: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 116: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 117: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 118: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 119: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 120: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 121: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 122: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 123: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 124: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 125: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 126: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 127: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 128: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 129: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 130: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 131: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 132: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 133: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 134: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 135: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 136: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 137: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 138: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 139: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 140: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 141: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 142: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 143: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 144: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 145: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 146: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 147: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 148: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 149: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 150: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 151: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 152: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 153: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 154: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 155: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 156: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 157: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 158: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 159: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 160: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 161: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 162: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 163: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 164: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 165: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 166: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 167: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 168: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 169: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 170: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 171: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 172: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 173: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 174: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 175: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 176: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 177: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 178: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 179: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 180: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 181: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 182: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 183: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 184: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 185: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 186: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 187: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 188: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 189: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 190: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 191: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 192: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 193: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 194: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 195: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 196: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 197: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 198: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 199: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 200: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 201: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 202: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 203: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 204: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 205: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 206: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 207: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 208: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 209: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 210: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 211: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 212: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 213: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 214: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 215: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 216: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 217: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 218: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 219: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 220: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 221: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 222: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 223: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 224: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 225: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 226: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 227: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 228: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 229: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 230: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 231: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 232: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 233: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 234: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 235: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 236: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 237: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 238: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 239: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 240: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 241: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 242: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 243: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 244: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 245: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 246: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 247: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 248: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 249: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 250: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 251: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 252: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 253: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 254: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 255: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 256: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 257: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 258: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 259: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 260: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 261: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 262: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 263: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 264: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 265: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 266: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 267: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 268: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 269: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 270: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 271: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 272: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 273: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 274: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 275: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 276: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 277: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 278: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 279: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 280: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 281: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 282: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 283: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 284: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 285: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 286: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 287: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 288: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 289: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 290: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 291: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 292: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 293: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 294: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 295: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 296: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 297: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 298: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 299: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n",
            "Iteration 300: x = 0.0000, Loss = 0.0000, Gradient = 0.0000\n"
          ]
        }
      ],
      "source": [
        "history = gradient_descent(starting_point, learning_rate, iterations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84b21797"
      },
      "source": [
        "#### Visualization of Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "70e9b233",
        "outputId": "504ff299-bfbe-4608-e4b0-d5c35afe319b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Extract x and loss values for plotting\n",
        "x_values, loss_values = zip(*history)\n",
        "\n",
        "# Plotting the Loss Function\n",
        "x_plot = np.linspace(-10, 10, 500)\n",
        "y_plot = loss_function(x_plot)\n",
        "\n",
        "# Enter your code\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_plot, y_plot, label='Loss Function', color='blue')\n",
        "plt.scatter(x_values, loss_values, color='red', label='Gradient Descent Points')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Gradient Descent Visualization')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3: Building a Shallow DNN in PyTorch\n",
        "Translate the mathematical primitives into a functional PyTorch model.\n",
        "\n",
        "![image](https://matthewmazur.files.wordpress.com/2018/03/neural_network-9.png)\n",
        "\n",
        "Define a class SimpleDNN that inherits from torch.nn.Module.\n",
        "Construct a simple network with two layers:\n",
        "- An input layer  a hidden layer (e.g., 2 input features, 2 hidden units).\n",
        "- A hidden layer  an output layer (e.g., 2 hidden units, 1 output unit).\n",
        "- Use the Sigmoid activation function on the output layer to ensure the output is between 0 and 1, suitable for binary classification probability.\n",
        "- Write the forward() method and test it with a sample input tensor (e.g., torch.Tensor([0.05, 0.10])).\n",
        "\n",
        "\n",
        "\n",
        "- [Source_blog](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)"
      ],
      "metadata": {
        "id": "af4OFjDUr_qA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cCudY0ifs3dn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "from sklearn import datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = nn.Linear(20, 30)"
      ],
      "metadata": {
        "id": "L94QrxabpjJQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn.Linear"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "ebDEQSxZpY-7",
        "outputId": "dfdac51b-6907-459c-b911-5fd2c3efca85"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.nn.modules.linear.Linear"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torch.nn.modules.linear.Linear</b><br/>def _wrapped_call_impl(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py</a>Applies an affine linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
              "\n",
              "This module supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.\n",
              "\n",
              "On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward.\n",
              "\n",
              "Args:\n",
              "    in_features: size of each input sample\n",
              "    out_features: size of each output sample\n",
              "    bias: If set to ``False``, the layer will not learn an additive bias.\n",
              "        Default: ``True``\n",
              "\n",
              "Shape:\n",
              "    - Input: :math:`(*, H_\\text{in})` where :math:`*` means any number of\n",
              "      dimensions including none and :math:`H_\\text{in} = \\text{in\\_features}`.\n",
              "    - Output: :math:`(*, H_\\text{out})` where all but the last dimension\n",
              "      are the same shape as the input and :math:`H_\\text{out} = \\text{out\\_features}`.\n",
              "\n",
              "Attributes:\n",
              "    weight: the learnable weights of the module of shape\n",
              "        :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
              "        initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
              "        :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
              "    bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
              "            If :attr:`bias` is ``True``, the values are initialized from\n",
              "            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
              "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
              "\n",
              "Examples::\n",
              "\n",
              "    &gt;&gt;&gt; m = nn.Linear(20, 30)\n",
              "    &gt;&gt;&gt; input = torch.randn(128, 20)\n",
              "    &gt;&gt;&gt; output = m(input)\n",
              "    &gt;&gt;&gt; print(output.size())\n",
              "    torch.Size([128, 30])</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 53);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = nn.Linear(2, 3)\n",
        "input = torch.randn(3, 2)\n",
        "output = m(input)\n"
      ],
      "metadata": {
        "id": "-s31FfWYgtJt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mT9MxDVgwpb",
        "outputId": "1f82859a-6f42-44fb-f88c-1f721b17906b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3995, 0.0991],\n",
              "        [0.0653, 0.4299],\n",
              "        [0.5749, 0.3880]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXzdqAxXgyhN",
        "outputId": "81cf40e6-e077-4098-d21f-75c085c6188e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.4339, -0.8475, -0.0108],\n",
              "        [ 0.1906, -0.7127,  0.0446],\n",
              "        [ 0.4880, -0.9511,  0.0968]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU_NBCL7g9Wf",
        "outputId": "01365e03-163a-44b3-a7e3-1eabf6f79312"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m.weight.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heEL947ehC4M",
        "outputId": "295f928b-0a70-4327-bef9-f911864e6cb4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.5705, -0.1592],\n",
              "        [-0.4735, -0.0708],\n",
              "        [ 0.1266,  0.2954]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m.bias.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wyFNG55hjon",
        "outputId": "27562ece-9646-4982-d1b9-bbc063227078"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.2218, -0.6514, -0.0906])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "GP9dhN6-s3do"
      },
      "outputs": [],
      "source": [
        "def init_weight(m):\n",
        "    if type(m) in [nn.Conv2d,nn.Linear]:\n",
        "        m.weight.data=torch.Tensor([[0.15,0.20],[0.25,0.30]])\n",
        "        m.bias.data=torch.Tensor([0.35])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "mb3BC7rfs3do"
      },
      "outputs": [],
      "source": [
        "## Model Set Up\n",
        "class Perceptron(nn.Module):\n",
        "  def __init__(self,input_size,H1,output_size):\n",
        "      super(Perceptron,self).__init__()\n",
        "      self.linear1 = nn.Linear(input_size, H1)\n",
        "      self.linear2 = nn.Linear(H1, output_size)\n",
        "\n",
        "      self.linear1.weight.data = torch.Tensor([[0.15, 0.20], [0.25, 0.30]])\n",
        "      self.linear1.bias.data = torch.Tensor([0.35, 0.35])\n",
        "      self.linear2.weight.data = torch.Tensor([[0.40, 0.45], [0.50, 0.55]])\n",
        "      self.linear2.bias.data = torch.Tensor([0.60, 0.60])\n",
        "\n",
        "  def forward(self,x, print_values=True):\n",
        "      net_h = self.linear1(x)\n",
        "      out_h = torch.sigmoid(net_h)\n",
        "\n",
        "      net_O = self.linear2(out_h)\n",
        "      out_O = torch.sigmoid(net_O)\n",
        "\n",
        "      if print_values:\n",
        "          print(f\"h1: {net_h[0]}, h2: {net_h[1]}\")\n",
        "          print(f\"out_h1: {out_h[0]}, out_h2: {out_h[1]}\")\n",
        "          print(f\"net_O1: {net_O[0]}, net_O2: {net_O[1]}\")\n",
        "          print(f\"out_O1: {out_O[0]}, out_O2: {out_O[1]}\")\n",
        "\n",
        "      return (out_O)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2Ed0JZUs3dp"
      },
      "source": [
        "## Unfold each epoch and check intermediate values\n",
        "\n",
        "\n",
        "### Initial Weight of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyNg0T9_s3dp",
        "outputId": "20776f0f-3acf-4591-fac8-9e7e0bf3e148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([[0.1500, 0.2000],\n",
            "        [0.2500, 0.3000]], requires_grad=True), Parameter containing:\n",
            "tensor([0.3500, 0.3500], requires_grad=True), Parameter containing:\n",
            "tensor([[0.4000, 0.4500],\n",
            "        [0.5000, 0.5500]], requires_grad=True), Parameter containing:\n",
            "tensor([0.6000, 0.6000], requires_grad=True)]\n"
          ]
        }
      ],
      "source": [
        "model=Perceptron(2,2,1)\n",
        "print (list(model.parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfiNDZJKs3dp"
      },
      "source": [
        "### After 1 forward pass check hidden, output and loss\n",
        "\n",
        "![image](https://matthewmazur.files.wordpress.com/2018/03/neural_network-9.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sG4sT_X1s3dp",
        "outputId": "77635d86-a563-4b48-f3c5-7aa799de6162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h1: 0.3774999976158142, h2: 0.39249998331069946\n",
            "out_h1: 0.5932700037956238, out_h2: 0.5968843698501587\n",
            "net_O1: 1.1059060096740723, net_O2: 1.224921464920044\n",
            "out_O1: 0.751365065574646, out_O2: 0.7729284763336182\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7514, 0.7729], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "model.forward(torch.Tensor([0.05,0.10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv_eHmLos3dq"
      },
      "source": [
        "$$\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = \\left( x_n - y_n \\right)^2,$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.MSELoss"
      ],
      "metadata": {
        "id": "EmRYUdNGjoaq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "10bf901f-7a71-4230-fdba-d75b42fa05df"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.nn.modules.loss.MSELoss"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torch.nn.modules.loss.MSELoss</b><br/>def _wrapped_call_impl(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py</a>Creates a criterion that measures the mean squared error (squared L2 norm) between\n",
              "each element in the input :math:`x` and target :math:`y`.\n",
              "\n",
              "The unreduced (i.e. with :attr:`reduction` set to ``&#x27;none&#x27;``) loss can be described as:\n",
              "\n",
              ".. math::\n",
              "    \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
              "    l_n = \\left( x_n - y_n \\right)^2,\n",
              "\n",
              "where :math:`N` is the batch size. If :attr:`reduction` is not ``&#x27;none&#x27;``\n",
              "(default ``&#x27;mean&#x27;``), then:\n",
              "\n",
              ".. math::\n",
              "    \\ell(x, y) =\n",
              "    \\begin{cases}\n",
              "        \\operatorname{mean}(L), &amp;  \\text{if reduction} = \\text{`mean&#x27;;}\\\\\n",
              "        \\operatorname{sum}(L),  &amp;  \\text{if reduction} = \\text{`sum&#x27;.}\n",
              "    \\end{cases}\n",
              "\n",
              ":math:`x` and :math:`y` are tensors of arbitrary shapes with a total\n",
              "of :math:`N` elements each.\n",
              "\n",
              "The mean operation still operates over all the elements, and divides by :math:`N`.\n",
              "\n",
              "The division by :math:`N` can be avoided if one sets ``reduction = &#x27;sum&#x27;``.\n",
              "\n",
              "Args:\n",
              "    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
              "        the losses are averaged over each loss element in the batch. Note that for\n",
              "        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n",
              "        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
              "        when :attr:`reduce` is ``False``. Default: ``True``\n",
              "    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
              "        losses are averaged or summed over observations for each minibatch depending\n",
              "        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
              "        batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
              "    reduction (str, optional): Specifies the reduction to apply to the output:\n",
              "        ``&#x27;none&#x27;`` | ``&#x27;mean&#x27;`` | ``&#x27;sum&#x27;``. ``&#x27;none&#x27;``: no reduction will be applied,\n",
              "        ``&#x27;mean&#x27;``: the sum of the output will be divided by the number of\n",
              "        elements in the output, ``&#x27;sum&#x27;``: the output will be summed. Note: :attr:`size_average`\n",
              "        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
              "        specifying either of those two args will override :attr:`reduction`. Default: ``&#x27;mean&#x27;``\n",
              "\n",
              "Shape:\n",
              "    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
              "    - Target: :math:`(*)`, same shape as the input.\n",
              "\n",
              "Examples:\n",
              "\n",
              "    &gt;&gt;&gt; loss = nn.MSELoss()\n",
              "    &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)\n",
              "    &gt;&gt;&gt; target = torch.randn(3, 5)\n",
              "    &gt;&gt;&gt; output = loss(input, target)\n",
              "    &gt;&gt;&gt; output.backward()</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 568);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOgSx-ams3dq",
        "outputId": "73f069d8-9cf1-4f9c-b644-1496aeabc645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h1: 0.3774999976158142, h2: 0.39249998331069946\n",
            "out_h1: 0.5932700037956238, out_h2: 0.5968843698501587\n",
            "net_O1: 1.1059060096740723, net_O2: 1.224921464920044\n",
            "out_O1: 0.751365065574646, out_O2: 0.7729284763336182\n",
            "total MSEerror: 0.2983711063861847\n"
          ]
        }
      ],
      "source": [
        "model=Perceptron(2,2,2)\n",
        "criterion=nn.MSELoss()\n",
        "optimizer=torch.optim.SGD(model.parameters(),lr=0.5)\n",
        "output=model.forward(torch.Tensor([0.05,0.10]))\n",
        "target=torch.Tensor([0.01,0.99])\n",
        "loss=criterion(output,target)\n",
        "print(\"total MSEerror: {}\".format(loss.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epoqeCqYs3dq"
      },
      "source": [
        "### Backward pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfesWBpVs3dq",
        "outputId": "55256ac1-d245-4e53-9553-3ba57f17daba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([[0.1498, 0.1996],\n",
            "        [0.2498, 0.2995]], requires_grad=True), Parameter containing:\n",
            "tensor([0.3456, 0.3450], requires_grad=True), Parameter containing:\n",
            "tensor([[0.3589, 0.4087],\n",
            "        [0.5113, 0.5614]], requires_grad=True), Parameter containing:\n",
            "tensor([0.5308, 0.6190], requires_grad=True)]\n"
          ]
        }
      ],
      "source": [
        "# set prev grad to zero\n",
        "optimizer.zero_grad()\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "optimizer.step()\n",
        "\n",
        "print(list(model.parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMRPiEjUs3dq"
      },
      "source": [
        "### 2nd Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "t4mXBhbYs3dq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3e075cf-a22a-4aba-af58-a05eb2901b5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h1: 0.37305948138237, h2: 0.3874606490135193\n",
            "out_h1: 0.5921980738639832, out_h2: 0.5956712365150452\n",
            "net_O1: 0.9867310523986816, net_O2: 1.256232738494873\n",
            "out_O1: 0.7284417748451233, out_O2: 0.7783768773078918\n"
          ]
        }
      ],
      "source": [
        "y_output = model.forward(torch.Tensor([0.05,0.10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkT_WtrSs3dq"
      },
      "source": [
        "### 2nd Backward pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nGajcYfs3dr",
        "outputId": "428778a2-1d40-466b-fff1-0570788ab28d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([[0.1496, 0.1992],\n",
            "        [0.2495, 0.2990]], requires_grad=True), Parameter containing:\n",
            "tensor([0.3417, 0.3405], requires_grad=True), Parameter containing:\n",
            "tensor([[0.3168, 0.3663],\n",
            "        [0.5221, 0.5722]], requires_grad=True), Parameter containing:\n",
            "tensor([0.4597, 0.6373], requires_grad=True)]\n"
          ]
        }
      ],
      "source": [
        "loss=criterion(y_output,target)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "optimizer.step()\n",
        "\n",
        "print(list(model.parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttqBL3LMs3dr"
      },
      "source": [
        "## Reinitializing the model and looping over Epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05EPxM6Ns3dr",
        "outputId": "0a67d8f1-b170-4b27-8e03-ddae26826603",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h1: 0.3774999976158142, h2: 0.39249998331069946\n",
            "out_h1: 0.5932700037956238, out_h2: 0.5968843698501587\n",
            "net_O1: 1.1059060096740723, net_O2: 1.224921464920044\n",
            "out_O1: 0.751365065574646, out_O2: 0.7729284763336182\n",
            "Epoch: 0, loss: 0.2983711063861847, output: tensor([0.7514, 0.7729], grad_fn=<SigmoidBackward0>)\n",
            "============================================================\n",
            "h1: 0.37305948138237, h2: 0.3874606490135193\n",
            "out_h1: 0.5921980738639832, out_h2: 0.5956712365150452\n",
            "net_O1: 0.9867310523986816, net_O2: 1.256232738494873\n",
            "out_O1: 0.7284417748451233, out_O2: 0.7783768773078918\n",
            "h1: 0.3691052794456482, h2: 0.3828778862953186\n",
            "out_h1: 0.591242790222168, out_h2: 0.5945670008659363\n",
            "net_O1: 0.8648310899734497, net_O2: 1.2862333059310913\n",
            "out_O1: 0.703669011592865, out_O2: 0.7835089564323425\n",
            "h1: 0.3657357096672058, h2: 0.37885743379592896\n",
            "out_h1: 0.5904281735420227, out_h2: 0.5935974717140198\n",
            "net_O1: 0.7411251068115234, net_O2: 1.3150603771209717\n",
            "out_O1: 0.677241861820221, out_O2: 0.7883586883544922\n",
            "h1: 0.3630351126194, h2: 0.3754920959472656\n",
            "out_h1: 0.5897749662399292, out_h2: 0.5927854180335999\n",
            "net_O1: 0.6167045831680298, net_O2: 1.3428380489349365\n",
            "out_O1: 0.649468719959259, out_O2: 0.7929562330245972\n",
            "h1: 0.36106494069099426, h2: 0.37285274267196655\n",
            "out_h1: 0.5892981886863708, out_h2: 0.5921481251716614\n",
            "net_O1: 0.4927763044834137, net_O2: 1.369676113128662\n",
            "out_O1: 0.6207602024078369, out_O2: 0.7973278164863586\n",
            "h1: 0.35985687375068665, h2: 0.3709806203842163\n",
            "out_h1: 0.5890057682991028, out_h2: 0.5916959047317505\n",
            "net_O1: 0.37058013677597046, net_O2: 1.3956677913665771\n",
            "out_O1: 0.59159916639328, out_O2: 0.8014955520629883\n",
            "h1: 0.35940971970558167, h2: 0.36988362669944763\n",
            "out_h1: 0.5888975262641907, out_h2: 0.5914308428764343\n",
            "net_O1: 0.25129494071006775, net_O2: 1.4208896160125732\n",
            "out_O1: 0.562495231628418, out_O2: 0.8054778575897217\n",
            "h1: 0.35969051718711853, h2: 0.36953675746917725\n",
            "out_h1: 0.5889655351638794, out_h2: 0.5913470387458801\n",
            "net_O1: 0.13595059514045715, net_O2: 1.445401906967163\n",
            "out_O1: 0.5339354276657104, out_O2: 0.8092897534370422\n",
            "h1: 0.360640287399292, h2: 0.36988726258277893\n",
            "out_h1: 0.5891954302787781, out_h2: 0.591431736946106\n",
            "net_O1: 0.025362901389598846, net_O2: 1.4692500829696655\n",
            "out_O1: 0.5063403844833374, out_O2: 0.8129433393478394\n",
            "h1: 0.3621819019317627, h2: 0.37086260318756104\n",
            "out_h1: 0.5895684957504272, out_h2: 0.5916673541069031\n",
            "net_O1: -0.07989799976348877, net_O2: 1.4924684762954712\n",
            "out_O1: 0.4800361096858978, out_O2: 0.816448450088501\n",
            "Epoch: 10, loss: 0.1255270540714264, output: tensor([0.4800, 0.8164], grad_fn=<SigmoidBackward0>)\n",
            "============================================================\n",
            "h1: 0.3642292618751526, h2: 0.37237921357154846\n",
            "out_h1: 0.5900638103485107, out_h2: 0.5920337438583374\n",
            "net_O1: -0.17950569093227386, net_O2: 1.515082836151123\n",
            "out_O1: 0.45524370670318604, out_O2: 0.8198132514953613\n",
            "h1: 0.366694837808609, h2: 0.3743506371974945\n",
            "out_h1: 0.5906600952148438, out_h2: 0.5925098061561584\n",
            "net_O1: -0.27334967255592346, net_O2: 1.5371129512786865\n",
            "out_O1: 0.43208494782447815, out_O2: 0.8230446577072144\n",
            "h1: 0.3694957494735718, h2: 0.3766936957836151\n",
            "out_h1: 0.5913371443748474, out_h2: 0.5930753946304321\n",
            "net_O1: -0.36149293184280396, net_O2: 1.5585757493972778\n",
            "out_O1: 0.4105982184410095, out_O2: 0.8261488676071167\n",
            "h1: 0.37255728244781494, h2: 0.37933242321014404\n",
            "out_h1: 0.5920767784118652, out_h2: 0.5937120914459229\n",
            "net_O1: -0.44412559270858765, net_O2: 1.579486608505249\n",
            "out_O1: 0.39075833559036255, out_O2: 0.8291317820549011\n",
            "h1: 0.37581467628479004, h2: 0.3822002112865448\n",
            "out_h1: 0.5928632020950317, out_h2: 0.5944036245346069\n",
            "net_O1: -0.521522045135498, net_O2: 1.599860429763794\n",
            "out_O1: 0.3724963665008545, out_O2: 0.831998884677887\n",
            "h1: 0.37921327352523804, h2: 0.38524022698402405\n",
            "out_h1: 0.5936833620071411, out_h2: 0.5951364040374756\n",
            "net_O1: -0.5940055847167969, net_O2: 1.619711995124817\n",
            "out_O1: 0.3557163178920746, out_O2: 0.8347553610801697\n",
            "h1: 0.3827081620693207, h2: 0.3884052336215973\n",
            "out_h1: 0.5945261120796204, out_h2: 0.5958986878395081\n",
            "net_O1: -0.6619211435317993, net_O2: 1.63905668258667\n",
            "out_O1: 0.34030818939208984, out_O2: 0.837406575679779\n",
            "h1: 0.38626301288604736, h2: 0.39165645837783813\n",
            "out_h1: 0.5953827500343323, out_h2: 0.5966814160346985\n",
            "net_O1: -0.7256165742874146, net_O2: 1.6579103469848633\n",
            "out_O1: 0.3261573910713196, out_O2: 0.8399572968482971\n",
            "h1: 0.38984882831573486, h2: 0.3949626684188843\n",
            "out_h1: 0.5962463021278381, out_h2: 0.5974768400192261\n",
            "net_O1: -0.7854295372962952, net_O2: 1.6762888431549072\n",
            "out_O1: 0.31315088272094727, out_O2: 0.8424124717712402\n",
            "h1: 0.3934428095817566, h2: 0.39829882979393005\n",
            "out_h1: 0.5971112251281738, out_h2: 0.5982788801193237\n",
            "net_O1: -0.8416800498962402, net_O2: 1.6942083835601807\n",
            "out_O1: 0.30118104815483093, out_O2: 0.8447767496109009\n",
            "Epoch: 20, loss: 0.052938103675842285, output: tensor([0.3012, 0.8448], grad_fn=<SigmoidBackward0>)\n",
            "============================================================\n",
            "h1: 0.39702722430229187, h2: 0.4016451835632324\n",
            "out_h1: 0.5979732275009155, out_h2: 0.599082887172699\n",
            "net_O1: -0.894666314125061, net_O2: 1.7116851806640625\n",
            "out_O1: 0.2901477813720703, out_O2: 0.8470547199249268\n",
            "h1: 0.40058836340904236, h2: 0.40498626232147217\n",
            "out_h1: 0.5988290309906006, out_h2: 0.5998850464820862\n",
            "net_O1: -0.9446629285812378, net_O2: 1.7287347316741943\n",
            "out_O1: 0.27995941042900085, out_O2: 0.8492505550384521\n",
            "h1: 0.4041158854961395, h2: 0.4083099663257599\n",
            "out_h1: 0.5996761322021484, out_h2: 0.6006825566291809\n",
            "net_O1: -0.9919207692146301, net_O2: 1.745373249053955\n",
            "out_O1: 0.27053284645080566, out_O2: 0.851368248462677\n",
            "h1: 0.4076019823551178, h2: 0.4116069972515106\n",
            "out_h1: 0.6005127429962158, out_h2: 0.6014731526374817\n",
            "net_O1: -1.036667823791504, net_O2: 1.7616157531738281\n",
            "out_O1: 0.26179346442222595, out_O2: 0.8534119129180908\n",
            "h1: 0.4110409617424011, h2: 0.41487032175064087\n",
            "out_h1: 0.6013374328613281, out_h2: 0.60225510597229\n",
            "net_O1: -1.0791103839874268, net_O2: 1.7774767875671387\n",
            "out_O1: 0.25367438793182373, out_O2: 0.8553850650787354\n",
            "h1: 0.4144287705421448, h2: 0.41809457540512085\n",
            "out_h1: 0.6021493077278137, out_h2: 0.6030272245407104\n",
            "net_O1: -1.1194350719451904, net_O2: 1.7929712533950806\n",
            "out_O1: 0.24611608684062958, out_O2: 0.8572911620140076\n",
            "h1: 0.41776254773139954, h2: 0.42127591371536255\n",
            "out_h1: 0.6029477119445801, out_h2: 0.6037885546684265\n",
            "net_O1: -1.1578102111816406, net_O2: 1.808112621307373\n",
            "out_O1: 0.23906540870666504, out_O2: 0.8591336011886597\n",
            "h1: 0.42104053497314453, h2: 0.42441150546073914\n",
            "out_h1: 0.6037322282791138, out_h2: 0.6045383810997009\n",
            "net_O1: -1.1943880319595337, net_O2: 1.8229143619537354\n",
            "out_O1: 0.23247507214546204, out_O2: 0.86091548204422\n",
            "h1: 0.4242616593837738, h2: 0.4274994730949402\n",
            "out_h1: 0.6045025587081909, out_h2: 0.6052764058113098\n",
            "net_O1: -1.2293055057525635, net_O2: 1.8373891115188599\n",
            "out_O1: 0.22630301117897034, out_O2: 0.8626396059989929\n",
            "h1: 0.4274255633354187, h2: 0.4305385947227478\n",
            "out_h1: 0.6052587628364563, out_h2: 0.6060022711753845\n",
            "net_O1: -1.2626867294311523, net_O2: 1.8515491485595703\n",
            "out_O1: 0.22051171958446503, out_O2: 0.8643088936805725\n",
            "Epoch: 30, loss: 0.030056718736886978, output: tensor([0.2205, 0.8643], grad_fn=<SigmoidBackward0>)\n",
            "============================================================\n",
            "h1: 0.4305322468280792, h2: 0.43352821469306946\n",
            "out_h1: 0.6060007810592651, out_h2: 0.6067159175872803\n",
            "net_O1: -1.2946438789367676, net_O2: 1.8654062747955322\n",
            "out_O1: 0.2150678038597107, out_O2: 0.8659258484840393\n",
            "h1: 0.4335820972919464, h2: 0.43646806478500366\n",
            "out_h1: 0.606728732585907, out_h2: 0.607417106628418\n",
            "net_O1: -1.3252785205841064, net_O2: 1.8789712190628052\n",
            "out_O1: 0.20994143187999725, out_O2: 0.8674929141998291\n",
            "h1: 0.4365758001804352, h2: 0.43935826420783997\n",
            "out_h1: 0.6074427962303162, out_h2: 0.6081061363220215\n",
            "net_O1: -1.3546829223632812, net_O2: 1.8922548294067383\n",
            "out_O1: 0.20510581135749817, out_O2: 0.8690124154090881\n",
            "h1: 0.43951424956321716, h2: 0.442199170589447\n",
            "out_h1: 0.6081432700157166, out_h2: 0.608782947063446\n",
            "net_O1: -1.3829407691955566, net_O2: 1.9052674770355225\n",
            "out_O1: 0.20053711533546448, out_O2: 0.8704865574836731\n",
            "h1: 0.44239845871925354, h2: 0.4449913203716278\n",
            "out_h1: 0.6088303923606873, out_h2: 0.609447717666626\n",
            "net_O1: -1.410128116607666, net_O2: 1.9180185794830322\n",
            "out_O1: 0.19621384143829346, out_O2: 0.8719172477722168\n",
            "h1: 0.44522956013679504, h2: 0.447735458612442\n",
            "out_h1: 0.6095044016838074, out_h2: 0.6101006865501404\n",
            "net_O1: -1.436314582824707, net_O2: 1.9305174350738525\n",
            "out_O1: 0.19211670756340027, out_O2: 0.8733066320419312\n",
            "h1: 0.4480087459087372, h2: 0.4504323899745941\n",
            "out_h1: 0.6101657152175903, out_h2: 0.6107420325279236\n",
            "net_O1: -1.4615635871887207, net_O2: 1.9427731037139893\n",
            "out_O1: 0.1882282942533493, out_O2: 0.8746564388275146\n",
            "h1: 0.4507372975349426, h2: 0.4530830383300781\n",
            "out_h1: 0.6108145117759705, out_h2: 0.6113719940185547\n",
            "net_O1: -1.4859333038330078, net_O2: 1.9547935724258423\n",
            "out_O1: 0.18453289568424225, out_O2: 0.8759683966636658\n",
            "h1: 0.4534164369106293, h2: 0.45568835735321045\n",
            "out_h1: 0.6114512085914612, out_h2: 0.6119908094406128\n",
            "net_O1: -1.509476661682129, net_O2: 1.9665870666503906\n",
            "out_O1: 0.18101635575294495, out_O2: 0.8772441148757935\n",
            "h1: 0.45604753494262695, h2: 0.4582493305206299\n",
            "out_h1: 0.6120761036872864, out_h2: 0.6125987768173218\n",
            "net_O1: -1.532242774963379, net_O2: 1.9781612157821655\n",
            "out_O1: 0.17766578495502472, out_O2: 0.8784850239753723\n",
            "Epoch: 40, loss: 0.02027370221912861, output: tensor([0.1777, 0.8785], grad_fn=<SigmoidBackward0>)\n",
            "============================================================\n",
            "h1: 0.45863184332847595, h2: 0.4607670307159424\n",
            "out_h1: 0.6126895546913147, out_h2: 0.6131961345672607\n",
            "net_O1: -1.554276466369629, net_O2: 1.9895235300064087\n",
            "out_O1: 0.1744694709777832, out_O2: 0.8796927332878113\n",
            "h1: 0.46117064356803894, h2: 0.4632425010204315\n",
            "out_h1: 0.6132918000221252, out_h2: 0.6137831211090088\n",
            "net_O1: -1.5756189823150635, net_O2: 2.000680446624756\n",
            "out_O1: 0.17141683399677277, out_O2: 0.8808685541152954\n",
            "h1: 0.4636651873588562, h2: 0.46567675471305847\n",
            "out_h1: 0.6138833165168762, out_h2: 0.6143600344657898\n",
            "net_O1: -1.596308708190918, net_O2: 2.011639356613159\n",
            "out_O1: 0.16849815845489502, out_O2: 0.882013738155365\n",
            "h1: 0.4661167860031128, h2: 0.4680708646774292\n",
            "out_h1: 0.6144642233848572, out_h2: 0.6149270534515381\n",
            "net_O1: -1.6163809299468994, net_O2: 2.0224061012268066\n",
            "out_O1: 0.16570459306240082, out_O2: 0.8831295967102051\n",
            "h1: 0.46852657198905945, h2: 0.4704258441925049\n",
            "out_h1: 0.6150349378585815, out_h2: 0.6154845356941223\n",
            "net_O1: -1.6358680725097656, net_O2: 2.032986640930176\n",
            "out_O1: 0.1630280762910843, out_O2: 0.8842171430587769\n",
            "h1: 0.47089576721191406, h2: 0.4727427363395691\n",
            "out_h1: 0.6155957579612732, out_h2: 0.6160327196121216\n",
            "net_O1: -1.6548006534576416, net_O2: 2.0433874130249023\n",
            "out_O1: 0.1604611724615097, out_O2: 0.8852777481079102\n",
            "h1: 0.47322553396224976, h2: 0.4750225245952606\n",
            "out_h1: 0.6161469221115112, out_h2: 0.616571843624115\n",
            "net_O1: -1.6732066869735718, net_O2: 2.0536131858825684\n",
            "out_O1: 0.15799710154533386, out_O2: 0.8863121867179871\n",
            "h1: 0.47551700472831726, h2: 0.4772661626338959\n",
            "out_h1: 0.6166887283325195, out_h2: 0.6171020865440369\n",
            "net_O1: -1.6911122798919678, net_O2: 2.0636701583862305\n",
            "out_O1: 0.15562961995601654, out_O2: 0.8873215913772583\n",
            "h1: 0.47777119278907776, h2: 0.47947463393211365\n",
            "out_h1: 0.6172214150428772, out_h2: 0.6176238059997559\n",
            "net_O1: -1.708541750907898, net_O2: 2.0735626220703125\n",
            "out_O1: 0.15335296094417572, out_O2: 0.8883069157600403\n",
            "h1: 0.479989230632782, h2: 0.48164886236190796\n",
            "out_h1: 0.6177453398704529, out_h2: 0.6181371212005615\n",
            "net_O1: -1.725517988204956, net_O2: 2.083296060562134\n",
            "out_O1: 0.1511617749929428, out_O2: 0.8892689943313599\n",
            "Epoch: 50, loss: 0.015036690980196, output: tensor([0.1512, 0.8893], grad_fn=<SigmoidBackward0>)\n",
            "============================================================\n",
            "h1: 0.4821721017360687, h2: 0.48378974199295044\n",
            "out_h1: 0.6182606220245361, out_h2: 0.6186423897743225\n",
            "net_O1: -1.7420618534088135, net_O2: 2.0928750038146973\n",
            "out_O1: 0.149051234126091, out_O2: 0.8902087807655334\n",
            "h1: 0.4843207895755768, h2: 0.4858981668949127\n",
            "out_h1: 0.6187676191329956, out_h2: 0.6191396713256836\n",
            "net_O1: -1.7581933736801147, net_O2: 2.1023035049438477\n",
            "out_O1: 0.1470167487859726, out_O2: 0.8911269307136536\n",
            "h1: 0.48643624782562256, h2: 0.48797497153282166\n",
            "out_h1: 0.6192665100097656, out_h2: 0.6196292638778687\n",
            "net_O1: -1.7739310264587402, net_O2: 2.111586093902588\n",
            "out_O1: 0.1450541466474533, out_O2: 0.8920242190361023\n",
            "h1: 0.4885194003582001, h2: 0.4900210201740265\n",
            "out_h1: 0.6197575926780701, out_h2: 0.6201114058494568\n",
            "net_O1: -1.7892920970916748, net_O2: 2.1207275390625\n",
            "out_O1: 0.14315953850746155, out_O2: 0.8929015398025513\n",
            "h1: 0.49057111144065857, h2: 0.4920370876789093\n",
            "out_h1: 0.6202409863471985, out_h2: 0.6205862164497375\n",
            "net_O1: -1.8042930364608765, net_O2: 2.1297309398651123\n",
            "out_O1: 0.14132928848266602, out_O2: 0.8937594294548035\n",
            "h1: 0.4925922453403473, h2: 0.4940239489078522\n",
            "out_h1: 0.62071692943573, out_h2: 0.6210538744926453\n",
            "net_O1: -1.8189489841461182, net_O2: 2.1386003494262695\n",
            "out_O1: 0.1395600289106369, out_O2: 0.894598662853241\n",
            "h1: 0.4945836663246155, h2: 0.4959823787212372\n",
            "out_h1: 0.6211856603622437, out_h2: 0.6215147376060486\n",
            "net_O1: -1.833274483680725, net_O2: 2.1473398208618164\n",
            "out_O1: 0.1378486454486847, out_O2: 0.8954198956489563\n",
            "h1: 0.4965461492538452, h2: 0.49791309237480164\n",
            "out_h1: 0.621647298336029, out_h2: 0.6219688057899475\n",
            "net_O1: -1.847282886505127, net_O2: 2.1559524536132812\n",
            "out_O1: 0.1361922323703766, out_O2: 0.8962236642837524\n",
            "h1: 0.4984804391860962, h2: 0.49981680512428284\n",
            "out_h1: 0.6221021413803101, out_h2: 0.6224162578582764\n",
            "net_O1: -1.8609869480133057, net_O2: 2.1644415855407715\n",
            "out_O1: 0.13458804786205292, out_O2: 0.8970105648040771\n",
            "h1: 0.5003873109817505, h2: 0.5016942024230957\n",
            "out_h1: 0.6225503087043762, out_h2: 0.622857391834259\n",
            "net_O1: -1.874398946762085, net_O2: 2.1728107929229736\n",
            "out_O1: 0.13303354382514954, out_O2: 0.8977811932563782\n",
            "Epoch: 60, loss: 0.011820781975984573, output: tensor([0.1330, 0.8978], grad_fn=<SigmoidBackward0>)\n",
            "============================================================\n",
            "h1: 0.5022674202919006, h2: 0.503545880317688\n",
            "out_h1: 0.6229920387268066, out_h2: 0.6232922673225403\n",
            "net_O1: -1.8875300884246826, net_O2: 2.181063175201416\n",
            "out_O1: 0.13152633607387543, out_O2: 0.8985360860824585\n",
            "h1: 0.5041215419769287, h2: 0.5053725838661194\n",
            "out_h1: 0.6234273910522461, out_h2: 0.6237210631370544\n",
            "net_O1: -1.900390863418579, net_O2: 2.1892011165618896\n",
            "out_O1: 0.1300642490386963, out_O2: 0.8992756009101868\n",
            "h1: 0.5059502124786377, h2: 0.5071747899055481\n",
            "out_h1: 0.6238566040992737, out_h2: 0.6241439580917358\n",
            "net_O1: -1.912992000579834, net_O2: 2.19722843170166\n",
            "out_O1: 0.12864509224891663, out_O2: 0.9000003337860107\n",
            "h1: 0.5077542662620544, h2: 0.5089532732963562\n",
            "out_h1: 0.6242798566818237, out_h2: 0.6245610117912292\n",
            "net_O1: -1.9253425598144531, net_O2: 2.2051472663879395\n",
            "out_O1: 0.127266988158226, out_O2: 0.9007108211517334\n",
            "h1: 0.5095341205596924, h2: 0.5107084512710571\n",
            "out_h1: 0.624697208404541, out_h2: 0.6249725818634033\n",
            "net_O1: -1.9374518394470215, net_O2: 2.212960720062256\n",
            "out_O1: 0.12592807412147522, out_O2: 0.9014073610305786\n",
            "h1: 0.5112904906272888, h2: 0.5124409794807434\n",
            "out_h1: 0.6251089572906494, out_h2: 0.6253785490989685\n",
            "net_O1: -1.9493284225463867, net_O2: 2.2206714153289795\n",
            "out_O1: 0.12462660670280457, out_O2: 0.9020904898643494\n",
            "h1: 0.5130239725112915, h2: 0.514151394367218\n",
            "out_h1: 0.6255151033401489, out_h2: 0.6257791519165039\n",
            "net_O1: -1.9609805345535278, net_O2: 2.228281259536743\n",
            "out_O1: 0.12336096912622452, out_O2: 0.9027605652809143\n",
            "h1: 0.5147350430488586, h2: 0.5158401727676392\n",
            "out_h1: 0.6259158253669739, out_h2: 0.6261745691299438\n",
            "net_O1: -1.9724161624908447, net_O2: 2.235793352127075\n",
            "out_O1: 0.12212961167097092, out_O2: 0.9034180641174316\n",
            "h1: 0.5164243578910828, h2: 0.5175078511238098\n",
            "out_h1: 0.6263113021850586, out_h2: 0.6265648603439331\n",
            "net_O1: -1.9836424589157104, net_O2: 2.2432096004486084\n",
            "out_O1: 0.12093108147382736, out_O2: 0.9040631651878357\n",
            "h1: 0.5180923342704773, h2: 0.5191548466682434\n",
            "out_h1: 0.6267015933990479, out_h2: 0.6269501447677612\n",
            "net_O1: -1.994666576385498, net_O2: 2.250532627105713\n",
            "out_O1: 0.11976402997970581, out_O2: 0.9046964645385742\n",
            "Epoch: 70, loss: 0.009662418626248837, output: tensor([0.1198, 0.9047], grad_fn=<SigmoidBackward0>)\n",
            "============================================================\n",
            "h1: 0.5197394490242004, h2: 0.5207817554473877\n",
            "out_h1: 0.6270868182182312, out_h2: 0.6273306012153625\n",
            "net_O1: -2.005495071411133, net_O2: 2.2577643394470215\n",
            "out_O1: 0.11862717568874359, out_O2: 0.9053182005882263\n",
            "h1: 0.5213662981987, h2: 0.5223889946937561\n",
            "out_h1: 0.6274672150611877, out_h2: 0.6277062296867371\n",
            "net_O1: -2.016134738922119, net_O2: 2.264906883239746\n",
            "out_O1: 0.11751926690340042, out_O2: 0.9059286117553711\n",
            "h1: 0.5229732990264893, h2: 0.5239769220352173\n",
            "out_h1: 0.6278427243232727, out_h2: 0.6280772089958191\n",
            "net_O1: -2.0265910625457764, net_O2: 2.2719621658325195\n",
            "out_O1: 0.11643917858600616, out_O2: 0.906528115272522\n",
            "h1: 0.5245609283447266, h2: 0.5255460739135742\n",
            "out_h1: 0.6282135844230652, out_h2: 0.628443717956543\n",
            "net_O1: -2.036870241165161, net_O2: 2.278932571411133\n",
            "out_O1: 0.11538581550121307, out_O2: 0.9071171879768372\n",
            "h1: 0.5261295437812805, h2: 0.5270968079566956\n",
            "out_h1: 0.6285799145698547, out_h2: 0.6288057565689087\n",
            "net_O1: -2.0469777584075928, net_O2: 2.2858197689056396\n",
            "out_O1: 0.11435811966657639, out_O2: 0.9076958298683167\n",
            "h1: 0.5276796817779541, h2: 0.5286295413970947\n",
            "out_h1: 0.6289417743682861, out_h2: 0.629163384437561\n",
            "net_O1: -2.0569188594818115, net_O2: 2.2926254272460938\n",
            "out_O1: 0.1133551299571991, out_O2: 0.9082644581794739\n",
            "h1: 0.5292116403579712, h2: 0.5301446914672852\n",
            "out_h1: 0.6292992234230042, out_h2: 0.6295168399810791\n",
            "net_O1: -2.0666987895965576, net_O2: 2.299351692199707\n",
            "out_O1: 0.112375907599926, out_O2: 0.9088233113288879\n",
            "h1: 0.5307259559631348, h2: 0.5316426157951355\n",
            "out_h1: 0.6296524405479431, out_h2: 0.6298661828041077\n",
            "net_O1: -2.076322078704834, net_O2: 2.306000232696533\n",
            "out_O1: 0.11141958087682724, out_O2: 0.9093727469444275\n",
            "h1: 0.5322229266166687, h2: 0.5331236720085144\n",
            "out_h1: 0.6300014853477478, out_h2: 0.630211353302002\n",
            "net_O1: -2.0857934951782227, net_O2: 2.312572479248047\n",
            "out_O1: 0.11048530042171478, out_O2: 0.9099130034446716\n",
            "h1: 0.5337029099464417, h2: 0.5345882177352905\n",
            "out_h1: 0.6303463578224182, out_h2: 0.6305525898933411\n",
            "net_O1: -2.0951175689697266, net_O2: 2.3190698623657227\n",
            "out_O1: 0.10957227647304535, out_O2: 0.9104440808296204\n",
            "Epoch: 80, loss: 0.008121891878545284, output: tensor([0.1096, 0.9104], grad_fn=<SigmoidBackward0>)\n",
            "============================================================\n",
            "h1: 0.535166323184967, h2: 0.5360366106033325\n",
            "out_h1: 0.6306872963905334, out_h2: 0.6308899521827698\n",
            "net_O1: -2.1042985916137695, net_O2: 2.3254947662353516\n",
            "out_O1: 0.10867972671985626, out_O2: 0.9109666347503662\n",
            "h1: 0.5366135239601135, h2: 0.537469208240509\n",
            "out_h1: 0.6310242414474487, out_h2: 0.6312234997749329\n",
            "net_O1: -2.113340139389038, net_O2: 2.33184814453125\n",
            "out_O1: 0.1078069731593132, out_O2: 0.9114805459976196\n",
            "h1: 0.538044810295105, h2: 0.5388862490653992\n",
            "out_h1: 0.6313574910163879, out_h2: 0.6315532922744751\n",
            "net_O1: -2.122246742248535, net_O2: 2.3381314277648926\n",
            "out_O1: 0.10695328563451767, out_O2: 0.911986231803894\n",
            "h1: 0.5394605994224548, h2: 0.5402881503105164\n",
            "out_h1: 0.6316869258880615, out_h2: 0.631879448890686\n",
            "net_O1: -2.1310219764709473, net_O2: 2.344346284866333\n",
            "out_O1: 0.10611800849437714, out_O2: 0.912483811378479\n",
            "h1: 0.5408610701560974, h2: 0.5416752099990845\n",
            "out_h1: 0.6320127248764038, out_h2: 0.6322020292282104\n",
            "net_O1: -2.139669179916382, net_O2: 2.350493907928467\n",
            "out_O1: 0.10530056059360504, out_O2: 0.9129734039306641\n",
            "h1: 0.5422466397285461, h2: 0.5430476069450378\n",
            "out_h1: 0.6323348879814148, out_h2: 0.6325210332870483\n",
            "net_O1: -2.1481921672821045, net_O2: 2.3565759658813477\n",
            "out_O1: 0.10450027883052826, out_O2: 0.9134554862976074\n",
            "h1: 0.5436175465583801, h2: 0.5444058179855347\n",
            "out_h1: 0.6326535940170288, out_h2: 0.6328367590904236\n",
            "net_O1: -2.1565942764282227, net_O2: 2.362593650817871\n",
            "out_O1: 0.10371662676334381, out_O2: 0.9139299988746643\n",
            "h1: 0.544974148273468, h2: 0.5457499623298645\n",
            "out_h1: 0.6329687833786011, out_h2: 0.6331489682197571\n",
            "net_O1: -2.1648781299591064, net_O2: 2.3685476779937744\n",
            "out_O1: 0.10294907540082932, out_O2: 0.9143972396850586\n",
            "h1: 0.5463166832923889, h2: 0.547080397605896\n",
            "out_h1: 0.6332806348800659, out_h2: 0.6334579586982727\n",
            "net_O1: -2.1730470657348633, net_O2: 2.3744399547576904\n",
            "out_O1: 0.10219711065292358, out_O2: 0.9148573875427246\n",
            "h1: 0.5476453900337219, h2: 0.5483973026275635\n",
            "out_h1: 0.6335891485214233, out_h2: 0.6337636709213257\n",
            "net_O1: -2.1811046600341797, net_O2: 2.3802716732025146\n",
            "out_O1: 0.10146017372608185, out_O2: 0.9153104424476624\n",
            "Epoch: 90, loss: 0.006971747614443302, output: tensor([0.1015, 0.9153], grad_fn=<SigmoidBackward0>)\n",
            "============================================================\n",
            "h1: 0.5489606261253357, h2: 0.5497010350227356\n",
            "out_h1: 0.6338943839073181, out_h2: 0.6340662240982056\n",
            "net_O1: -2.1890528202056885, net_O2: 2.3860435485839844\n",
            "out_O1: 0.10073786228895187, out_O2: 0.9157568216323853\n",
            "h1: 0.5502626299858093, h2: 0.5509918928146362\n",
            "out_h1: 0.6341965198516846, out_h2: 0.6343656778335571\n",
            "net_O1: -2.196895122528076, net_O2: 2.391757011413574\n",
            "out_O1: 0.10002966225147247, out_O2: 0.9161965847015381\n",
            "h1: 0.5515516400337219, h2: 0.5522699356079102\n",
            "out_h1: 0.6344954967498779, out_h2: 0.6346620917320251\n",
            "net_O1: -2.2046337127685547, net_O2: 2.3974132537841797\n",
            "out_O1: 0.09933514893054962, out_O2: 0.9166298508644104\n",
            "h1: 0.5528278946876526, h2: 0.5535355806350708\n",
            "out_h1: 0.6347914934158325, out_h2: 0.6349555253982544\n",
            "net_O1: -2.212271213531494, net_O2: 2.403013229370117\n",
            "out_O1: 0.09865392744541168, out_O2: 0.9170567393302917\n",
            "h1: 0.5540916323661804, h2: 0.5547888875007629\n",
            "out_h1: 0.635084331035614, out_h2: 0.6352459192276001\n",
            "net_O1: -2.2198100090026855, net_O2: 2.408557891845703\n",
            "out_O1: 0.09798559546470642, out_O2: 0.917477548122406\n",
            "h1: 0.5553430914878845, h2: 0.5560302138328552\n",
            "out_h1: 0.6353743672370911, out_h2: 0.6355335116386414\n",
            "net_O1: -2.22725248336792, net_O2: 2.414048671722412\n",
            "out_O1: 0.0973297655582428, out_O2: 0.917892336845398\n",
            "h1: 0.556582510471344, h2: 0.5572597980499268\n",
            "out_h1: 0.6356614232063293, out_h2: 0.6358183026313782\n",
            "net_O1: -2.2346014976501465, net_O2: 2.4194858074188232\n",
            "out_O1: 0.09668600559234619, out_O2: 0.9183012247085571\n",
            "h1: 0.5578100681304932, h2: 0.5584777593612671\n",
            "out_h1: 0.6359456777572632, out_h2: 0.6361002922058105\n",
            "net_O1: -2.24185848236084, net_O2: 2.4248709678649902\n",
            "out_O1: 0.09605405479669571, out_O2: 0.9187042713165283\n",
            "h1: 0.5590260624885559, h2: 0.5596843361854553\n",
            "out_h1: 0.6362271308898926, out_h2: 0.6363794803619385\n",
            "net_O1: -2.249025821685791, net_O2: 2.4302046298980713\n",
            "out_O1: 0.09543352574110031, out_O2: 0.9191017746925354\n",
            "h1: 0.5602306723594666, h2: 0.5608797669410706\n",
            "out_h1: 0.6365059018135071, out_h2: 0.6366561055183411\n",
            "net_O1: -2.256105899810791, net_O2: 2.435488224029541\n",
            "out_O1: 0.09482408314943314, out_O2: 0.9194937944412231\n",
            "Epoch: 100, loss: 0.006083126179873943, output: tensor([0.0948, 0.9195], grad_fn=<SigmoidBackward0>)\n",
            "============================================================\n",
            "h1: 0.5614240765571594, h2: 0.5620641708374023\n",
            "out_h1: 0.6367819905281067, out_h2: 0.6369300484657288\n",
            "net_O1: -2.2631003856658936, net_O2: 2.4407219886779785\n",
            "out_O1: 0.0942254289984703, out_O2: 0.9198803901672363\n",
            "h1: 0.5626064538955688, h2: 0.5632377862930298\n",
            "out_h1: 0.6370553970336914, out_h2: 0.6372013688087463\n",
            "net_O1: -2.2700109481811523, net_O2: 2.4459071159362793\n",
            "out_O1: 0.09363728016614914, out_O2: 0.9202616214752197\n",
            "h1: 0.5637779831886292, h2: 0.5644007921218872\n",
            "out_h1: 0.6373262405395508, out_h2: 0.6374701857566833\n",
            "net_O1: -2.2768402099609375, net_O2: 2.451044797897339\n",
            "out_O1: 0.09305929392576218, out_O2: 0.920637845993042\n",
            "h1: 0.5649389624595642, h2: 0.5655534267425537\n",
            "out_h1: 0.6375945210456848, out_h2: 0.6377365589141846\n",
            "net_O1: -2.2835893630981445, net_O2: 2.4561352729797363\n",
            "out_O1: 0.09249123185873032, out_O2: 0.9210090041160583\n",
            "h1: 0.5660895109176636, h2: 0.5666957497596741\n",
            "out_h1: 0.6378603577613831, out_h2: 0.6380004286766052\n",
            "net_O1: -2.2902603149414062, net_O2: 2.461179733276367\n",
            "out_O1: 0.09193281829357147, out_O2: 0.9213752150535583\n",
            "h1: 0.5672298073768616, h2: 0.5678280591964722\n",
            "out_h1: 0.6381237506866455, out_h2: 0.6382618546485901\n",
            "net_O1: -2.2968549728393555, net_O2: 2.4661786556243896\n",
            "out_O1: 0.09138377010822296, out_O2: 0.9217365384101868\n",
            "h1: 0.5683600902557373, h2: 0.5689504146575928\n",
            "out_h1: 0.6383846402168274, out_h2: 0.6385209560394287\n",
            "net_O1: -2.3033745288848877, net_O2: 2.471133232116699\n",
            "out_O1: 0.09084387123584747, out_O2: 0.9220931529998779\n",
            "h1: 0.5694804191589355, h2: 0.570063054561615\n",
            "out_h1: 0.6386433243751526, out_h2: 0.6387777328491211\n",
            "net_O1: -2.309821128845215, net_O2: 2.476043701171875\n",
            "out_O1: 0.09031283855438232, out_O2: 0.9224452376365662\n",
            "h1: 0.5705909729003906, h2: 0.5711660981178284\n",
            "out_h1: 0.638899564743042, out_h2: 0.639032244682312\n",
            "net_O1: -2.3161957263946533, net_O2: 2.4809112548828125\n",
            "out_O1: 0.08979049324989319, out_O2: 0.9227927923202515\n",
            "h1: 0.5716919302940369, h2: 0.572259783744812\n",
            "out_h1: 0.6391534805297852, out_h2: 0.6392844319343567\n",
            "net_O1: -2.322500228881836, net_O2: 2.485736131668091\n",
            "out_O1: 0.08927655965089798, out_O2: 0.9231358170509338\n",
            "Epoch: 110, loss: 0.005377796478569508, output: tensor([0.0893, 0.9231], grad_fn=<SigmoidBackward0>)\n",
            "============================================================\n",
            "h1: 0.5727834701538086, h2: 0.5733441114425659\n",
            "out_h1: 0.6394051909446716, out_h2: 0.6395344138145447\n",
            "net_O1: -2.328735828399658, net_O2: 2.4905195236206055\n",
            "out_O1: 0.08877087384462357, out_O2: 0.9234745502471924\n",
            "h1: 0.5738657712936401, h2: 0.574419379234314\n",
            "out_h1: 0.6396546959877014, out_h2: 0.6397823095321655\n",
            "net_O1: -2.334904193878174, net_O2: 2.4952616691589355\n",
            "out_O1: 0.08827317506074905, out_O2: 0.9238089919090271\n",
            "h1: 0.5749388933181763, h2: 0.5754856467247009\n",
            "out_h1: 0.6399019956588745, out_h2: 0.6400279998779297\n",
            "net_O1: -2.3410065174102783, net_O2: 2.4999637603759766\n",
            "out_O1: 0.08778328448534012, out_O2: 0.9241392612457275\n",
            "h1: 0.5760031342506409, h2: 0.5765430927276611\n",
            "out_h1: 0.6401472091674805, out_h2: 0.6402716040611267\n",
            "net_O1: -2.3470442295074463, net_O2: 2.5046257972717285\n",
            "out_O1: 0.08730100095272064, out_O2: 0.9244654774665833\n",
            "h1: 0.5770584344863892, h2: 0.5775918364524841\n",
            "out_h1: 0.6403902769088745, out_h2: 0.6405131220817566\n",
            "net_O1: -2.3530185222625732, net_O2: 2.509248971939087\n",
            "out_O1: 0.0868261456489563, out_O2: 0.9247876405715942\n",
            "h1: 0.578105092048645, h2: 0.5786320567131042\n",
            "out_h1: 0.6406312584877014, out_h2: 0.6407526135444641\n",
            "net_O1: -2.3589305877685547, net_O2: 2.513833522796631\n",
            "out_O1: 0.08635853230953217, out_O2: 0.9251059293746948\n",
            "h1: 0.579143226146698, h2: 0.5796638131141663\n",
            "out_h1: 0.640870213508606, out_h2: 0.6409900784492493\n",
            "net_O1: -2.3647818565368652, net_O2: 2.5183801651000977\n",
            "out_O1: 0.08589798212051392, out_O2: 0.925420343875885\n",
            "h1: 0.5801728963851929, h2: 0.580687403678894\n",
            "out_h1: 0.6411072015762329, out_h2: 0.6412255764007568\n",
            "net_O1: -2.3705732822418213, net_O2: 2.5228893756866455\n",
            "out_O1: 0.08544433116912842, out_O2: 0.9257309436798096\n",
            "h1: 0.5811943411827087, h2: 0.581702709197998\n",
            "out_h1: 0.6413421630859375, out_h2: 0.6414591073989868\n",
            "net_O1: -2.3763060569763184, net_O2: 2.5273618698120117\n",
            "out_O1: 0.08499740809202194, out_O2: 0.9260377883911133\n",
            "h1: 0.5822076201438904, h2: 0.5827099680900574\n",
            "out_h1: 0.641575276851654, out_h2: 0.641690731048584\n",
            "net_O1: -2.381981134414673, net_O2: 2.5317983627319336\n",
            "out_O1: 0.08455708622932434, out_O2: 0.9263411164283752\n",
            "Epoch: 120, loss: 0.0048056067898869514, output: tensor([0.0846, 0.9263], grad_fn=<SigmoidBackward0>)\n",
            "============================================================\n",
            "h1: 0.5832127928733826, h2: 0.5837093591690063\n",
            "out_h1: 0.6418063640594482, out_h2: 0.6419205069541931\n",
            "net_O1: -2.3875999450683594, net_O2: 2.5361990928649902\n",
            "out_O1: 0.0841231644153595, out_O2: 0.9266408085823059\n",
            "h1: 0.5842100381851196, h2: 0.5847010016441345\n",
            "out_h1: 0.6420355439186096, out_h2: 0.6421483755111694\n",
            "net_O1: -2.3931632041931152, net_O2: 2.540564775466919\n",
            "out_O1: 0.08369552344083786, out_O2: 0.9269370436668396\n",
            "h1: 0.5851995348930359, h2: 0.5856848955154419\n",
            "out_h1: 0.6422629356384277, out_h2: 0.6423744559288025\n",
            "net_O1: -2.398672580718994, net_O2: 2.544896125793457\n",
            "out_O1: 0.0832739770412445, out_O2: 0.9272298812866211\n",
            "h1: 0.5861814022064209, h2: 0.5866612195968628\n",
            "out_h1: 0.6424884796142578, out_h2: 0.6425986886024475\n",
            "net_O1: -2.404128074645996, net_O2: 2.5491931438446045\n",
            "out_O1: 0.08285845071077347, out_O2: 0.9275193214416504\n",
            "h1: 0.5871556997299194, h2: 0.587630033493042\n",
            "out_h1: 0.6427122950553894, out_h2: 0.642821192741394\n",
            "net_O1: -2.409531593322754, net_O2: 2.5534567832946777\n",
            "out_O1: 0.08244875073432922, out_O2: 0.9278054237365723\n",
            "h1: 0.5881224870681763, h2: 0.5885916352272034\n",
            "out_h1: 0.642934262752533, out_h2: 0.6430419087409973\n",
            "net_O1: -2.414883613586426, net_O2: 2.557687282562256\n",
            "out_O1: 0.08204476535320282, out_O2: 0.9280882477760315\n",
            "h1: 0.5890819430351257, h2: 0.5895459055900574\n",
            "out_h1: 0.6431544423103333, out_h2: 0.6432609558105469\n",
            "net_O1: -2.420185089111328, net_O2: 2.561884880065918\n",
            "out_O1: 0.0816463753581047, out_O2: 0.9283679127693176\n",
            "h1: 0.5900341272354126, h2: 0.5904930830001831\n",
            "out_h1: 0.6433729529380798, out_h2: 0.643478274345398\n",
            "net_O1: -2.4254369735717773, net_O2: 2.5660510063171387\n",
            "out_O1: 0.08125345408916473, out_O2: 0.9286444783210754\n",
            "h1: 0.5909792184829712, h2: 0.5914331674575806\n",
            "out_h1: 0.6435897946357727, out_h2: 0.6436939239501953\n",
            "net_O1: -2.4306397438049316, net_O2: 2.5701851844787598\n",
            "out_O1: 0.08086590468883514, out_O2: 0.9289178848266602\n",
            "h1: 0.5919172763824463, h2: 0.5923663973808289\n",
            "out_h1: 0.6438049077987671, out_h2: 0.643907904624939\n",
            "net_O1: -2.4357948303222656, net_O2: 2.5742883682250977\n",
            "out_O1: 0.0804835706949234, out_O2: 0.9291884303092957\n",
            "Epoch: 130, loss: 0.004332991316914558, output: tensor([0.0805, 0.9292], grad_fn=<SigmoidBackward0>)\n",
            "============================================================\n",
            "h1: 0.5928484201431274, h2: 0.5932927131652832\n",
            "out_h1: 0.6440184116363525, out_h2: 0.6441202759742737\n",
            "net_O1: -2.4409027099609375, net_O2: 2.5783605575561523\n",
            "out_O1: 0.08010637015104294, out_O2: 0.9294558763504028\n",
            "h1: 0.5937727093696594, h2: 0.5942122936248779\n",
            "out_h1: 0.644230306148529, out_h2: 0.6443310379981995\n",
            "net_O1: -2.445964813232422, net_O2: 2.582402467727661\n",
            "out_O1: 0.07973413914442062, out_O2: 0.929720401763916\n",
            "h1: 0.5946902632713318, h2: 0.5951251983642578\n",
            "out_h1: 0.6444405317306519, out_h2: 0.6445402503013611\n",
            "net_O1: -2.4509811401367188, net_O2: 2.586414337158203\n",
            "out_O1: 0.07936683297157288, out_O2: 0.9299821257591248\n",
            "h1: 0.5956012010574341, h2: 0.5960316061973572\n",
            "out_h1: 0.6446492671966553, out_h2: 0.6447478532791138\n",
            "net_O1: -2.4559526443481445, net_O2: 2.5903971195220947\n",
            "out_O1: 0.07900433242321014, out_O2: 0.9302409887313843\n",
            "h1: 0.5965055227279663, h2: 0.596931517124176\n",
            "out_h1: 0.6448563933372498, out_h2: 0.6449539661407471\n",
            "net_O1: -2.4608802795410156, net_O2: 2.594350576400757\n",
            "out_O1: 0.07864652574062347, out_O2: 0.9304971098899841\n",
            "h1: 0.5974034070968628, h2: 0.5978250503540039\n",
            "out_h1: 0.6450619697570801, out_h2: 0.6451585292816162\n",
            "net_O1: -2.4657645225524902, net_O2: 2.5982754230499268\n",
            "out_O1: 0.0782933384180069, out_O2: 0.9307505488395691\n",
            "h1: 0.5982949733734131, h2: 0.5987123250961304\n",
            "out_h1: 0.6452661156654358, out_h2: 0.6453616619110107\n",
            "net_O1: -2.4706063270568848, net_O2: 2.6021721363067627\n",
            "out_O1: 0.07794464379549026, out_O2: 0.9310012459754944\n",
            "h1: 0.5991801619529724, h2: 0.5995934009552002\n",
            "out_h1: 0.6454687118530273, out_h2: 0.6455633044242859\n",
            "net_O1: -2.4754064083099365, net_O2: 2.6060409545898438\n",
            "out_O1: 0.07760036736726761, out_O2: 0.9312493801116943\n",
            "h1: 0.6000592112541199, h2: 0.6004682779312134\n",
            "out_h1: 0.6456698775291443, out_h2: 0.6457634568214417\n",
            "net_O1: -2.480165481567383, net_O2: 2.60988187789917\n",
            "out_O1: 0.07726040482521057, out_O2: 0.9314948916435242\n",
            "h1: 0.6009321212768555, h2: 0.6013371348381042\n",
            "out_h1: 0.6458695530891418, out_h2: 0.6459621787071228\n",
            "net_O1: -2.4848837852478027, net_O2: 2.6136960983276367\n",
            "out_O1: 0.07692470401525497, out_O2: 0.9317378401756287\n",
            "Epoch: 140, loss: 0.003936698194593191, output: tensor([0.0769, 0.9317], grad_fn=<SigmoidBackward0>)\n",
            "============================================================\n",
            "h1: 0.6017990112304688, h2: 0.6021999716758728\n",
            "out_h1: 0.6460677981376648, out_h2: 0.6461594700813293\n",
            "net_O1: -2.489562511444092, net_O2: 2.617483377456665\n",
            "out_O1: 0.07659313082695007, out_O2: 0.9319782853126526\n",
            "h1: 0.6026599407196045, h2: 0.6030570268630981\n",
            "out_h1: 0.6462646126747131, out_h2: 0.646355390548706\n",
            "net_O1: -2.49420166015625, net_O2: 2.621244192123413\n",
            "out_O1: 0.07626566290855408, out_O2: 0.9322164058685303\n",
            "h1: 0.6035149693489075, h2: 0.6039081811904907\n",
            "out_h1: 0.6464600563049316, out_h2: 0.6465499401092529\n",
            "net_O1: -2.498802661895752, net_O2: 2.624979019165039\n",
            "out_O1: 0.07594215869903564, out_O2: 0.9324519038200378\n",
            "h1: 0.6043642163276672, h2: 0.6047536134719849\n",
            "out_h1: 0.6466541290283203, out_h2: 0.64674311876297\n",
            "net_O1: -2.5033652782440186, net_O2: 2.628688097000122\n",
            "out_O1: 0.07562259584665298, out_O2: 0.9326851963996887\n",
            "h1: 0.6052077412605286, h2: 0.6055933833122253\n",
            "out_h1: 0.6468468308448792, out_h2: 0.6469349265098572\n",
            "net_O1: -2.5078907012939453, net_O2: 2.632371425628662\n",
            "out_O1: 0.07530686259269714, out_O2: 0.9329161047935486\n",
            "h1: 0.6060455441474915, h2: 0.6064276099205017\n",
            "out_h1: 0.6470381617546082, out_h2: 0.6471254229545593\n",
            "net_O1: -2.5123791694641113, net_O2: 2.6360301971435547\n",
            "out_O1: 0.07499489933252335, out_O2: 0.9331446886062622\n",
            "h1: 0.606877863407135, h2: 0.607256293296814\n",
            "out_h1: 0.6472283005714417, out_h2: 0.6473146677017212\n",
            "net_O1: -2.516831398010254, net_O2: 2.6396636962890625\n",
            "out_O1: 0.07468663156032562, out_O2: 0.9333710670471191\n",
            "h1: 0.6077046394348145, h2: 0.6080794930458069\n",
            "out_h1: 0.6474170088768005, out_h2: 0.6475025415420532\n",
            "net_O1: -2.5212478637695312, net_O2: 2.643272876739502\n",
            "out_O1: 0.074381984770298, out_O2: 0.9335951209068298\n",
            "h1: 0.6085259914398193, h2: 0.6088973879814148\n",
            "out_h1: 0.6476044654846191, out_h2: 0.6476892232894897\n",
            "net_O1: -2.5256295204162598, net_O2: 2.6468582153320312\n",
            "out_O1: 0.07408086955547333, out_O2: 0.9338170886039734\n",
            "h1: 0.6093419194221497, h2: 0.6097099184989929\n",
            "out_h1: 0.6477906703948975, out_h2: 0.6478745937347412\n",
            "net_O1: -2.5299763679504395, net_O2: 2.6504197120666504\n",
            "out_O1: 0.0737832635641098, out_O2: 0.9340367913246155\n",
            "Epoch: 150, loss: 0.0036000933032482862, output: tensor([0.0738, 0.9340], grad_fn=<SigmoidBackward0>)\n",
            "============================================================\n",
            "h1: 0.6101526021957397, h2: 0.6105172038078308\n",
            "out_h1: 0.6479756236076355, out_h2: 0.6480588316917419\n",
            "net_O1: -2.5342893600463867, net_O2: 2.6539576053619385\n",
            "out_O1: 0.07348905503749847, out_O2: 0.93425452709198\n",
            "h1: 0.6109580397605896, h2: 0.6113193035125732\n",
            "out_h1: 0.6481593251228333, out_h2: 0.6482416987419128\n",
            "net_O1: -2.5385680198669434, net_O2: 2.6574721336364746\n",
            "out_O1: 0.07319825887680054, out_O2: 0.9344700574874878\n",
            "h1: 0.6117582321166992, h2: 0.612116277217865\n",
            "out_h1: 0.6483417749404907, out_h2: 0.6484233736991882\n",
            "net_O1: -2.542814016342163, net_O2: 2.660963535308838\n",
            "out_O1: 0.07291073352098465, out_O2: 0.9346835017204285\n",
            "h1: 0.6125533580780029, h2: 0.6129081845283508\n",
            "out_h1: 0.6485230326652527, out_h2: 0.6486038565635681\n",
            "net_O1: -2.547027349472046, net_O2: 2.6644325256347656\n",
            "out_O1: 0.07262644171714783, out_O2: 0.9348949790000916\n",
            "h1: 0.6133434772491455, h2: 0.6136951446533203\n",
            "out_h1: 0.6487030982971191, out_h2: 0.648783266544342\n",
            "net_O1: -2.55120849609375, net_O2: 2.667879104614258\n",
            "out_O1: 0.07234533876180649, out_O2: 0.935104489326477\n",
            "h1: 0.6141285300254822, h2: 0.6144771575927734\n",
            "out_h1: 0.6488820314407349, out_h2: 0.6489614248275757\n",
            "net_O1: -2.5553579330444336, net_O2: 2.6713032722473145\n",
            "out_O1: 0.07206735759973526, out_O2: 0.9353119730949402\n",
            "h1: 0.614908754825592, h2: 0.615254282951355\n",
            "out_h1: 0.6490597724914551, out_h2: 0.6491383910179138\n",
            "net_O1: -2.559476137161255, net_O2: 2.674705982208252\n",
            "out_O1: 0.07179243862628937, out_O2: 0.9355175495147705\n",
            "h1: 0.6156840920448303, h2: 0.6160265803337097\n",
            "out_h1: 0.6492363214492798, out_h2: 0.6493143439292908\n",
            "net_O1: -2.563563346862793, net_O2: 2.678086757659912\n",
            "out_O1: 0.07152055203914642, out_O2: 0.9357211589813232\n",
            "h1: 0.616454541683197, h2: 0.6167941093444824\n",
            "out_h1: 0.6494117975234985, out_h2: 0.6494891047477722\n",
            "net_O1: -2.5676205158233643, net_O2: 2.6814463138580322\n",
            "out_O1: 0.07125160098075867, out_O2: 0.9359229207038879\n",
            "h1: 0.6172202825546265, h2: 0.6175569295883179\n",
            "out_h1: 0.6495860815048218, out_h2: 0.6496627330780029\n",
            "net_O1: -2.5716476440429688, net_O2: 2.6847848892211914\n",
            "out_O1: 0.0709855705499649, out_O2: 0.9361228346824646\n",
            "Epoch: 160, loss: 0.0033109949436038733, output: tensor([0.0710, 0.9361], grad_fn=<SigmoidBackward0>)\n",
            "============================================================\n",
            "h1: 0.6179813146591187, h2: 0.6183151006698608\n",
            "out_h1: 0.6497592926025391, out_h2: 0.6498352885246277\n",
            "net_O1: -2.5756449699401855, net_O2: 2.6881024837493896\n",
            "out_O1: 0.07072240859270096, out_O2: 0.936320960521698\n",
            "h1: 0.6187377572059631, h2: 0.6190686821937561\n",
            "out_h1: 0.6499314308166504, out_h2: 0.6500067114830017\n",
            "net_O1: -2.579613208770752, net_O2: 2.691399574279785\n",
            "out_O1: 0.07046206295490265, out_O2: 0.9365172386169434\n",
            "h1: 0.6194896101951599, h2: 0.6198177337646484\n",
            "out_h1: 0.650102436542511, out_h2: 0.6501770615577698\n",
            "net_O1: -2.583552837371826, net_O2: 2.694676399230957\n",
            "out_O1: 0.07020445913076401, out_O2: 0.9367117881774902\n",
            "h1: 0.6202369332313538, h2: 0.6205623745918274\n",
            "out_h1: 0.6502724289894104, out_h2: 0.6503464579582214\n",
            "net_O1: -2.5874643325805664, net_O2: 2.6979329586029053\n",
            "out_O1: 0.06994956731796265, out_O2: 0.9369046092033386\n",
            "h1: 0.6209796667098999, h2: 0.6213024854660034\n",
            "out_h1: 0.6504413485527039, out_h2: 0.6505147218704224\n",
            "net_O1: -2.5913476943969727, net_O2: 2.701169729232788\n",
            "out_O1: 0.06969735026359558, out_O2: 0.937095582485199\n",
            "h1: 0.6217180490493774, h2: 0.6220381855964661\n",
            "out_h1: 0.6506091952323914, out_h2: 0.6506819725036621\n",
            "net_O1: -2.595203161239624, net_O2: 2.7043867111206055\n",
            "out_O1: 0.06944777816534042, out_O2: 0.9372850060462952\n",
            "h1: 0.6224520206451416, h2: 0.6227695941925049\n",
            "out_h1: 0.6507759690284729, out_h2: 0.6508482098579407\n",
            "net_O1: -2.599031686782837, net_O2: 2.7075843811035156\n",
            "out_O1: 0.06920076161623001, out_O2: 0.9374727010726929\n",
            "h1: 0.6231817007064819, h2: 0.6234966516494751\n",
            "out_h1: 0.6509418487548828, out_h2: 0.6510133743286133\n",
            "net_O1: -2.6028332710266113, net_O2: 2.7107629776000977\n",
            "out_O1: 0.06895630061626434, out_O2: 0.9376587271690369\n",
            "h1: 0.6239071488380432, h2: 0.624219536781311\n",
            "out_h1: 0.6511066555976868, out_h2: 0.6511775851249695\n",
            "net_O1: -2.6066086292266846, net_O2: 2.7139225006103516\n",
            "out_O1: 0.06871430575847626, out_O2: 0.9378432035446167\n",
            "h1: 0.6246283054351807, h2: 0.6249382495880127\n",
            "out_h1: 0.6512704491615295, out_h2: 0.6513408422470093\n",
            "net_O1: -2.6103577613830566, net_O2: 2.7170629501342773\n",
            "out_O1: 0.06847477704286575, out_O2: 0.9380260109901428\n",
            "Epoch: 170, loss: 0.003060298040509224, output: tensor([0.0685, 0.9380], grad_fn=<SigmoidBackward0>)\n",
            "============================================================\n",
            "h1: 0.6253452897071838, h2: 0.6256527900695801\n",
            "out_h1: 0.6514332890510559, out_h2: 0.6515030860900879\n",
            "net_O1: -2.6140809059143066, net_O2: 2.7201852798461914\n",
            "out_O1: 0.06823767721652985, out_O2: 0.9382072687149048\n",
            "h1: 0.6260581016540527, h2: 0.626363217830658\n",
            "out_h1: 0.6515951156616211, out_h2: 0.6516643762588501\n",
            "net_O1: -2.617778778076172, net_O2: 2.7232890129089355\n",
            "out_O1: 0.06800293922424316, out_O2: 0.9383869767189026\n",
            "h1: 0.6267668604850769, h2: 0.6270695924758911\n",
            "out_h1: 0.6517559885978699, out_h2: 0.6518247127532959\n",
            "net_O1: -2.6214513778686523, net_O2: 2.726374626159668\n",
            "out_O1: 0.06777054071426392, out_O2: 0.9385651350021362\n",
            "h1: 0.6274715662002563, h2: 0.6277719736099243\n",
            "out_h1: 0.651915967464447, out_h2: 0.6519840955734253\n",
            "net_O1: -2.6250991821289062, net_O2: 2.7294421195983887\n",
            "out_O1: 0.06754044443368912, out_O2: 0.9387418031692505\n",
            "h1: 0.6281722187995911, h2: 0.6284703612327576\n",
            "out_h1: 0.6520748734474182, out_h2: 0.6521425247192383\n",
            "net_O1: -2.6287221908569336, net_O2: 2.732491970062256\n",
            "out_O1: 0.06731262803077698, out_O2: 0.9389169812202454\n",
            "h1: 0.6288689374923706, h2: 0.6291647553443909\n",
            "out_h1: 0.6522329449653625, out_h2: 0.6523000597953796\n",
            "net_O1: -2.63232159614563, net_O2: 2.7355246543884277\n",
            "out_O1: 0.06708700209856033, out_O2: 0.9390906095504761\n",
            "h1: 0.6295617818832397, h2: 0.6298553943634033\n",
            "out_h1: 0.6523901224136353, out_h2: 0.6524566411972046\n",
            "net_O1: -2.635896682739258, net_O2: 2.738539457321167\n",
            "out_O1: 0.06686359643936157, out_O2: 0.9392628073692322\n",
            "h1: 0.6302507519721985, h2: 0.6305420994758606\n",
            "out_h1: 0.6525462865829468, out_h2: 0.6526123881340027\n",
            "net_O1: -2.6394481658935547, net_O2: 2.741537094116211\n",
            "out_O1: 0.06664235144853592, out_O2: 0.9394336342811584\n",
            "h1: 0.6309358477592468, h2: 0.6312249898910522\n",
            "out_h1: 0.6527016758918762, out_h2: 0.6527671813964844\n",
            "net_O1: -2.642976760864258, net_O2: 2.7445180416107178\n",
            "out_O1: 0.0664232075214386, out_O2: 0.9396029710769653\n",
            "h1: 0.6316171288490295, h2: 0.6319041848182678\n",
            "out_h1: 0.6528560519218445, out_h2: 0.6529211401939392\n",
            "net_O1: -2.646481990814209, net_O2: 2.7474820613861084\n",
            "out_O1: 0.06620617210865021, out_O2: 0.9397709369659424\n",
            "Epoch: 180, loss: 0.0028410470113158226, output: tensor([0.0662, 0.9398], grad_fn=<SigmoidBackward0>)\n",
            "============================================================\n",
            "h1: 0.6322946548461914, h2: 0.6325796246528625\n",
            "out_h1: 0.6530095934867859, out_h2: 0.6530741453170776\n",
            "net_O1: -2.6499643325805664, net_O2: 2.750429391860962\n",
            "out_O1: 0.06599120795726776, out_O2: 0.9399375319480896\n",
            "h1: 0.6329684853553772, h2: 0.6332513689994812\n",
            "out_h1: 0.6531622409820557, out_h2: 0.6532263159751892\n",
            "net_O1: -2.6534245014190674, net_O2: 2.7533602714538574\n",
            "out_O1: 0.06577825546264648, out_O2: 0.9401028156280518\n",
            "h1: 0.6336386203765869, h2: 0.6339194774627686\n",
            "out_h1: 0.6533140540122986, out_h2: 0.6533776521682739\n",
            "net_O1: -2.656862735748291, net_O2: 2.756274700164795\n",
            "out_O1: 0.06556728482246399, out_O2: 0.9402667880058289\n",
            "h1: 0.6343051195144653, h2: 0.6345840096473694\n",
            "out_h1: 0.6534649729728699, out_h2: 0.6535281538963318\n",
            "net_O1: -2.660278797149658, net_O2: 2.7591733932495117\n",
            "out_O1: 0.06535829603672028, out_O2: 0.9404293298721313\n",
            "h1: 0.634968101978302, h2: 0.6352449655532837\n",
            "out_h1: 0.6536151170730591, out_h2: 0.6536778211593628\n",
            "net_O1: -2.6636734008789062, net_O2: 2.7620558738708496\n",
            "out_O1: 0.06515124440193176, out_O2: 0.9405906796455383\n",
            "h1: 0.6356274485588074, h2: 0.6359023451805115\n",
            "out_h1: 0.6537644267082214, out_h2: 0.6538265943527222\n",
            "net_O1: -2.667046308517456, net_O2: 2.764922618865967\n",
            "out_O1: 0.06494610756635666, out_O2: 0.9407505989074707\n",
            "h1: 0.636283278465271, h2: 0.6365562081336975\n",
            "out_h1: 0.6539127826690674, out_h2: 0.6539745926856995\n",
            "net_O1: -2.670397996902466, net_O2: 2.7677736282348633\n",
            "out_O1: 0.06474286317825317, out_O2: 0.9409092664718628\n",
            "h1: 0.6369355916976929, h2: 0.6372066736221313\n",
            "out_h1: 0.654060423374176, out_h2: 0.6541217565536499\n",
            "net_O1: -2.6737289428710938, net_O2: 2.770609140396118\n",
            "out_O1: 0.06454146653413773, out_O2: 0.9410667419433594\n",
            "h1: 0.6375845074653625, h2: 0.6378536820411682\n",
            "out_h1: 0.6542072892189026, out_h2: 0.6542680859565735\n",
            "net_O1: -2.677039384841919, net_O2: 2.7734296321868896\n",
            "out_O1: 0.06434188038110733, out_O2: 0.9412230253219604\n",
            "h1: 0.6382299065589905, h2: 0.6384972929954529\n",
            "out_h1: 0.6543532013893127, out_h2: 0.654413640499115\n",
            "net_O1: -2.6803293228149414, net_O2: 2.7762346267700195\n",
            "out_O1: 0.06414410471916199, out_O2: 0.9413779973983765\n",
            "Epoch: 190, loss: 0.0026478422805666924, output: tensor([0.0641, 0.9414], grad_fn=<SigmoidBackward0>)\n",
            "============================================================\n",
            "h1: 0.638871967792511, h2: 0.6391375660896301\n",
            "out_h1: 0.6544983983039856, out_h2: 0.6545584797859192\n",
            "net_O1: -2.6835992336273193, net_O2: 2.779024600982666\n",
            "out_O1: 0.06394809484481812, out_O2: 0.9415317177772522\n",
            "h1: 0.6395107507705688, h2: 0.6397745013237\n",
            "out_h1: 0.6546428799629211, out_h2: 0.6547024846076965\n",
            "net_O1: -2.6868491172790527, net_O2: 2.7818002700805664\n",
            "out_O1: 0.06375383585691452, out_O2: 0.941684365272522\n",
            "h1: 0.6401461362838745, h2: 0.6404081583023071\n",
            "out_h1: 0.6547865271568298, out_h2: 0.6548457145690918\n",
            "net_O1: -2.6900794506073, net_O2: 2.7845609188079834\n",
            "out_O1: 0.06356129050254822, out_O2: 0.9418357610702515\n",
            "h1: 0.6407782435417175, h2: 0.6410385966300964\n",
            "out_h1: 0.6549293994903564, out_h2: 0.654988169670105\n",
            "net_O1: -2.6932902336120605, net_O2: 2.787306785583496\n",
            "out_O1: 0.06337044388055801, out_O2: 0.9419860243797302\n",
            "h1: 0.6414071321487427, h2: 0.6416657567024231\n",
            "out_h1: 0.6550714373588562, out_h2: 0.6551299095153809\n",
            "net_O1: -2.696481704711914, net_O2: 2.790038585662842\n",
            "out_O1: 0.06318128108978271, out_O2: 0.9421350955963135\n",
            "h1: 0.64203280210495, h2: 0.6422896981239319\n",
            "out_h1: 0.6552128195762634, out_h2: 0.6552708745002747\n",
            "net_O1: -2.6996541023254395, net_O2: 2.7927560806274414\n",
            "out_O1: 0.06299376487731934, out_O2: 0.9422831535339355\n",
            "h1: 0.6426552534103394, h2: 0.6429104804992676\n",
            "out_h1: 0.6553534269332886, out_h2: 0.6554111242294312\n",
            "net_O1: -2.702807903289795, net_O2: 2.795459270477295\n",
            "out_O1: 0.06280787289142609, out_O2: 0.9424300193786621\n",
            "h1: 0.6432745456695557, h2: 0.643528163433075\n",
            "out_h1: 0.6554933190345764, out_h2: 0.6555505990982056\n",
            "net_O1: -2.7059428691864014, net_O2: 2.7981486320495605\n",
            "out_O1: 0.06262359023094177, out_O2: 0.9425756931304932\n",
            "h1: 0.6438907384872437, h2: 0.644142746925354\n",
            "out_h1: 0.6556324362754822, out_h2: 0.6556893587112427\n",
            "net_O1: -2.709059238433838, net_O2: 2.800823926925659\n",
            "out_O1: 0.0624408982694149, out_O2: 0.9427202939987183\n"
          ]
        }
      ],
      "source": [
        "model=Perceptron(2,2,2)\n",
        "criterion=nn.MSELoss()\n",
        "optimizer=torch.optim.SGD(model.parameters(),lr=0.5)\n",
        "\n",
        "input_x = torch.Tensor([0.05,0.10])\n",
        "epoch = 200\n",
        "for i in range(epoch):\n",
        "    # Forward Pass\n",
        "    y_output = model.forward(input_x)\n",
        "\n",
        "    # Loss\n",
        "    loss=criterion(y_output,target)\n",
        "\n",
        "    # Backward Pass\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if i%10 == 0:\n",
        "        print(f'Epoch: {i}, loss: {loss.item()}, output: {y_output}')\n",
        "        print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tr9pGF5_s3dr"
      },
      "source": [
        "**Note:** at Line 3 above, if we use `Adam` optimizer, it will learn faster in lesser epoch. For example, with Adam, only using 10 epochs, the model will learn the optimum weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWF8DTQus3dr",
        "outputId": "831e2f91-8319-4369-e887-febddaf945af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([[0.1632, 0.2264],\n",
            "        [0.2625, 0.3249]], requires_grad=True), Parameter containing:\n",
            "tensor([0.6137, 0.5991], requires_grad=True), Parameter containing:\n",
            "tensor([[-0.9046, -0.8579],\n",
            "        [ 1.0152,  1.0661]], requires_grad=True), Parameter containing:\n",
            "tensor([-1.5563,  1.4385], requires_grad=True)]\n"
          ]
        }
      ],
      "source": [
        "print(list(model.parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rON6T98Ns3dr"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgU9YQlrs3dr",
        "outputId": "9b13e727-99b2-4cb6-9de9-61cf6d181537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h1: 0.6506630778312683, h2: 0.6538770198822021\n",
            "out_h1: 0.6571599245071411, out_h2: 0.6578835844993591\n",
            "net_O1: -2.7151784896850586, net_O2: 2.8070878982543945\n",
            "out_O1: 0.06208362430334091, out_O2: 0.943057656288147\n",
            "tensor([0.0621, 0.9431], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print (model.forward(torch.Tensor([0.06,0.12])))"
      ]
    }
  ]
}